name: Benchmarks

on:
  schedule:
    # Run benchmarks every Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      compare_branch:
        description: 'Branch to compare against (leave empty for no comparison)'
        required: false
        default: ''
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'include/**'
      - 'benchmarks/**'

jobs:
  benchmark-suite:
    name: Full Benchmark Suite
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        config:
          - name: "Dynamic MT"
            flags: "-DSS_ENABLE_THREAD_SAFETY=1"
          - name: "Dynamic ST"
            flags: "-DSS_ENABLE_THREAD_SAFETY=0"
          - name: "Static MT"
            flags: "-DSS_USE_STATIC_MEMORY=1 -DSS_MAX_SIGNALS=256 -DSS_MAX_SLOTS=1024 -DSS_ENABLE_THREAD_SAFETY=1"
          - name: "Static ST"
            flags: "-DSS_USE_STATIC_MEMORY=1 -DSS_MAX_SIGNALS=256 -DSS_MAX_SLOTS=1024 -DSS_ENABLE_THREAD_SAFETY=0"
          - name: "Minimal"
            flags: "-DSS_MINIMAL_BUILD -DSS_USE_STATIC_MEMORY=1 -DSS_MAX_SIGNALS=32 -DSS_MAX_SLOTS=128"
        exclude:
          # Skip some Windows combinations to save time
          - os: windows-latest
            config:
              name: "Static ST"
          - os: windows-latest
            config:
              name: "Minimal"
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup MSYS2 (Windows)
      if: matrix.os == 'windows-latest'
      uses: msys2/setup-msys2@v2
      with:
        msystem: MINGW64
        update: true
        install: >-
          mingw-w64-x86_64-gcc
          make
    
    - name: Build benchmark (Unix)
      if: matrix.os != 'windows-latest'
      run: |
        mkdir -p build
        gcc -O3 -march=native ${{ matrix.config.flags }} \
          -Iinclude \
          benchmarks/benchmark_ss_lib.c \
          src/ss_lib_v2.c \
          -pthread -lm \
          -o build/benchmark_${{ matrix.config.name }}_ss_lib
    
    - name: Build benchmark (Windows)
      if: matrix.os == 'windows-latest'
      shell: msys2 {0}
      run: |
        mkdir -p build
        gcc -O3 ${{ matrix.config.flags }} \
          -Iinclude \
          benchmarks/benchmark_ss_lib.c \
          src/ss_lib_v2.c \
          -pthread -lm \
          -o build/benchmark_${{ matrix.config.name }}_ss_lib.exe
    
    - name: Run benchmark (Unix)
      if: matrix.os != 'windows-latest'
      run: |
        echo "=== ${{ matrix.config.name }} Configuration ===" | tee -a results_${{ matrix.os }}.txt
        ./build/benchmark_${{ matrix.config.name }}_ss_lib | tee -a results_${{ matrix.os }}.txt
        echo "" | tee -a results_${{ matrix.os }}.txt
    
    - name: Run benchmark (Windows)
      if: matrix.os == 'windows-latest'
      shell: msys2 {0}
      run: |
        echo "=== ${{ matrix.config.name }} Configuration ===" | tee -a results_${{ matrix.os }}.txt
        ./build/benchmark_${{ matrix.config.name }}_ss_lib.exe | tee -a results_${{ matrix.os }}.txt
        echo "" | tee -a results_${{ matrix.os }}.txt
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.os }}
        path: results_${{ matrix.os }}.txt

  benchmark-report:
    name: Generate Benchmark Report
    runs-on: ubuntu-latest
    needs: benchmark-suite
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download all results
      uses: actions/download-artifact@v3
      with:
        path: results/
    
    - name: Generate report
      run: |
        echo "# SS_Lib Benchmark Report" > BENCHMARK_REPORT.md
        echo "" >> BENCHMARK_REPORT.md
        echo "Generated: $(date -u)" >> BENCHMARK_REPORT.md
        echo "" >> BENCHMARK_REPORT.md
        
        for os in ubuntu-latest macos-latest windows-latest; do
          if [ -f "results/benchmark-results-$os/results_$os.txt" ]; then
            echo "## $os" >> BENCHMARK_REPORT.md
            echo "" >> BENCHMARK_REPORT.md
            echo '```' >> BENCHMARK_REPORT.md
            cat "results/benchmark-results-$os/results_$os.txt" >> BENCHMARK_REPORT.md
            echo '```' >> BENCHMARK_REPORT.md
            echo "" >> BENCHMARK_REPORT.md
          fi
        done
        
        # Create summary table
        cat > summarize.py << 'EOF'
        import re
        import os
        
        results = {}
        
        for root, dirs, files in os.walk('results'):
            for file in files:
                if file.endswith('.txt'):
                    os_name = file.replace('results_', '').replace('.txt', '')
                    config = None
                    
                    with open(os.path.join(root, file), 'r') as f:
                        for line in f:
                            if '=== ' in line and ' Configuration ===' in line:
                                config = line.strip().replace('=== ', '').replace(' Configuration ===', '')
                            elif 'Emit void signal (no slots)' in line and 'avg=' in line:
                                match = re.search(r'avg=\s*(\d+)\s*ns', line)
                                if match and config:
                                    key = f"{os_name}/{config}"
                                    results[key] = int(match.group(1))
        
        print("\n## Summary: Emit void signal (no slots) - Average Time (ns)\n")
        print("| OS / Configuration | Time (ns) |")
        print("|-------------------|-----------|")
        
        for key in sorted(results.keys()):
            print(f"| {key} | {results[key]} |")
        EOF
        
        python3 summarize.py >> BENCHMARK_REPORT.md || echo "Failed to generate summary" >> BENCHMARK_REPORT.md
    
    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-report
        path: BENCHMARK_REPORT.md
    
    - name: Update benchmark results in repo
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        mkdir -p docs/benchmarks
        cp BENCHMARK_REPORT.md docs/benchmarks/latest.md
        
        # Keep historical record
        cp BENCHMARK_REPORT.md "docs/benchmarks/$(date +%Y-%m-%d).md"
        
        # Create index
        echo "# Benchmark History" > docs/benchmarks/index.md
        echo "" >> docs/benchmarks/index.md
        echo "- [Latest](latest.md)" >> docs/benchmarks/index.md
        echo "" >> docs/benchmarks/index.md
        echo "## Historical Results" >> docs/benchmarks/index.md
        echo "" >> docs/benchmarks/index.md
        
        for file in docs/benchmarks/*.md; do
          if [[ $file =~ [0-9]{4}-[0-9]{2}-[0-9]{2}\.md$ ]]; then
            basename=$(basename "$file" .md)
            echo "- [$basename]($basename.md)" >> docs/benchmarks/index.md
          fi
        done
        
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add docs/benchmarks/
        git diff --staged --quiet || git commit -m "Update benchmark results [skip ci]"
        git push

  performance-regression:
    name: Check Performance Regression
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Run benchmarks on PR branch
      run: |
        make clean
        make benchmark
        ./build/benchmark_ss_lib > pr_results.txt
    
    - name: Run benchmarks on base branch
      run: |
        git checkout ${{ github.event.pull_request.base.sha }}
        make clean
        make benchmark
        ./build/benchmark_ss_lib > base_results.txt
    
    - name: Check for regressions
      run: |
        cat > check_regression.py << 'EOF'
        import re
        import sys
        
        def parse_results(filename):
            results = {}
            with open(filename, 'r') as f:
                for line in f:
                    match = re.match(r'^(.+?):\s*avg=\s*(\d+)\s*ns', line)
                    if match:
                        name = match.group(1).strip()
                        avg = int(match.group(2))
                        results[name] = avg
            return results
        
        base = parse_results('base_results.txt')
        pr = parse_results('pr_results.txt')
        
        regression_found = False
        threshold = 10  # 10% regression threshold
        
        print("Performance Regression Check")
        print("=" * 50)
        print()
        
        for name in base:
            if name in pr:
                base_time = base[name]
                pr_time = pr[name]
                change = ((pr_time - base_time) / base_time) * 100
                
                if change > threshold:
                    print(f"❌ REGRESSION: {name}")
                    print(f"   Base: {base_time} ns")
                    print(f"   PR:   {pr_time} ns")
                    print(f"   Change: {change:+.1f}%")
                    print()
                    regression_found = True
                elif change < -5:
                    print(f"✅ IMPROVEMENT: {name}")
                    print(f"   Base: {base_time} ns")
                    print(f"   PR:   {pr_time} ns")
                    print(f"   Change: {change:+.1f}%")
                    print()
        
        if regression_found:
            print("\n⚠️  Performance regressions detected!")
            sys.exit(1)
        else:
            print("\n✅ No significant performance regressions detected.")
        EOF
        
        python3 check_regression.py